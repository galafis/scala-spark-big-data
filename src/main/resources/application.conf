spark {
  app-name = "Scala Spark Big Data Processing"
  master = "local[*]"
  log-level = "WARN"
  enable-hive = true
  warehouse-dir = "spark-warehouse"
  checkpoint-dir = "checkpoints"
  shuffle-partitions = 200
  default-parallelism = 8
  
  # AWS S3 configuration (uncomment and fill to use)
  # aws {
  #   access-key = "your-access-key"
  #   secret-key = "your-secret-key"
  #   s3-endpoint = "s3.amazonaws.com"
  # }
}

data {
  input-path = "data/input"
  output-path = "data/output"
  temp-path = "data/temp"
  format = "parquet"
  partition-columns = ["year", "month", "day"]
}

app {
  batch-size = 10000
  processing-mode = "batch"  # Options: batch, streaming
  enable-caching = true
  num-retries = 3
  retry-interval = 5000  # milliseconds
}

