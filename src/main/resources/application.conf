# =============================================================================
# Scala Spark Big Data Platform - Configuration File
# =============================================================================
# This configuration file contains all settings for the Big Data platform
# including Spark optimizations, environment settings, and cloud integrations.
# =============================================================================
# Environment Configuration
# =============================================================================
environment = "local"  # Options: local, cluster, cloud

# =============================================================================
# Apache Spark Core Configuration
# =============================================================================
spark {
  # Application settings
  app-name = "Scala Spark Big Data Processing"

  # Memory and CPU configuration
  executor.memory = "4g"
  executor.cores = "2"
  executor.instances = "2"
  driver.memory = "2g"
  driver.maxResultSize = "1g"

  # Shuffle optimization
  sql.shuffle.partitions = "200"

  # Master URL for different environments
  master = "local[*]"  # Will be overridden by environment-specific config

  # Kubernetes configuration (for cluster mode)
  kubernetes.image = "apache/spark:latest"

  # Hive and warehouse settings
  enable-hive = true
  warehouse-dir = "spark-warehouse"
  checkpoint-dir = "checkpoints"
  log-level = "WARN"

  # Additional Spark optimizations from README
  serializer = "org.apache.spark.serializer.KryoSerializer"
  sql.adaptive.enabled = true
  sql.adaptive.coalescePartitions.enabled = true
  sql.adaptive.skewJoin.enabled = true
  sql.codegen.wholeStage = true
  sql.codegen.factoryMode = "CODEGEN_ONLY"
  sql.inMemoryColumnarStorage.compressed = true
  sql.inMemoryColumnarStorage.batchSize = 20000
  shuffle.service.enabled = true
  shuffle.compress = true
  shuffle.spill.compress = true
}

# =============================================================================
# Data Lake Integration Settings (README: Delta Lake and Iceberg)
# =============================================================================
delta {
  enabled = true
  # Spark session extensions and catalog will be applied by code when enabled
}
iceberg {
  enabled = false
}

# =============================================================================
# Cloud Provider Configuration (README: AWS/GCP/Azure support)
# =============================================================================
cloud {
  provider = "aws"  # Options: aws, gcp, azure
}

# AWS Configuration (README + SparkSessionManager expected keys)
aws {
  region = "us-east-1"
  s3.bucket = "your-s3-bucket"
  s3-endpoint = "s3.amazonaws.com"
  # Credentials typically provided via environment/instance profile. Placeholders for local/dev only.
  access-key = "your-access-key"
  secret-key = "your-secret-key"
}

# GCP Configuration
gcp {
  project-id = "your-project-id"
  region = "us-central1"
  storage.bucket = "your-gcs-bucket"
}

# Azure Configuration
azure {
  subscription-id = "your-subscription-id"
  resource-group = "your-resource-group"
  storage.account = "your-storage-account"
}

# =============================================================================
# Data Processing / ETL Configuration (README: ETL pipelines, quality, lineage)
# =============================================================================
data {
  input-path = "data/input"
  output-path = "data/output"
  temp-path = "data/temp"
  format = "parquet"
  partition-columns = ["year", "month", "day"]

  # Optional schema and lineage/validation toggles referenced by README modules
  schema.path = "data/schemas"
  validation.enabled = true
  lineage.enabled = true
}

# =============================================================================
# Streaming Configuration (README: Kafka + Kinesis)
# =============================================================================
streaming {
  # General
  checkpoint.location = "checkpoints/streaming"
  batch.interval = "10 seconds"   # micro-batch trigger default

  kafka {
    bootstrap.servers = "localhost:9092"
    group.id = "spark-streaming-group"
    auto.offset.reset = "latest"
    # Extra options aligned with README sample code
    session.timeout.ms = 30000
    request.timeout.ms = 40000
    max.poll.records = 1000
    fetch.max.wait.ms = 500
    fail.on.data.loss = false
    topics = ["events"]
  }

  # AWS Kinesis (mentioned in README)
  kinesis {
    enabled = false
    stream.name = "your-kinesis-stream"
    region = ${aws.region}
    initial.position = "LATEST"  # TRIM_HORIZON or LATEST
    checkpoint.interval = "1 minute"
  }
}

# =============================================================================
# Output Path Configuration for Streaming Sinks (README usage)
# =============================================================================
output {
  raw_events_path = "data/raw_events"
  aggregations_path = "data/aggregations"
  sessions_path = "data/user_sessions"
  anomalies_path = "data/anomalies"
}

# =============================================================================
# Application Settings (README: performance, retries, quality thresholds)
# =============================================================================
app {
  batch-size = 10000
  processing-mode = "batch"  # Options: batch, streaming
  enable-caching = true
  num-retries = 3
  retry-interval = 5000  # milliseconds

  # Performance tuning
  cache-storage-level = "MEMORY_AND_DISK_SER"
  optimization.enabled = true

  # Data quality thresholds used by DataQuality/DataValidation modules
  data-quality {
    null-threshold = 0.5
    outlier-threshold = 3.0
  }
}

# =============================================================================
# Machine Learning Configuration (README: pipelines, ALS, RF)
# =============================================================================
ml {
  model-path = "models"
  feature-store-path = "feature-store"

  # Default hyperparameters
  random-forest {
    num-trees = 100
    max-depth = 10
    min-instances-per-node = 1
  }

  recommendation {
    rank = 50
    max-iter = 20
    reg-param = 0.1
    alpha = 1.0
    seed = 42
    cold-start-strategy = "drop"
  }
}

# =============================================================================
# Monitoring and Logging (README: Prometheus + Grafana)
# =============================================================================
monitoring {
  metrics.enabled = true
  health-check.interval = "30 seconds"

  prometheus {
    enabled = true
    port = 9090
  }

  grafana {
    enabled = true
    port = 3000
  }

  # Optional alerts integration stub (README mentions AlertManager)
  alerting {
    enabled = false
    channel = "stdout"
  }
}

# =============================================================================
# Security Configuration (README: Kerberos, encryption, SSL, access control)
# =============================================================================
security {
  encryption.enabled = false
  kerberos.enabled = false
  ssl.enabled = false
  # Optional access control toggles used by security modules
  access-control.enabled = false
  audit.enabled = false
}
