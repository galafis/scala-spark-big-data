# =============================================================================
# Scala Spark Big Data Platform - Configuration File
# =============================================================================
# This configuration file contains all settings for the Big Data platform
# including Spark optimizations, environment settings, and cloud integrations.

# =============================================================================
# Environment Configuration
# =============================================================================
environment = "local"  # Options: local, cluster, cloud

# =============================================================================
# Apache Spark Core Configuration
# =============================================================================
spark {
  # Application settings
  app-name = "Scala Spark Big Data Processing"
  
  # Memory and CPU configuration
  executor.memory = "4g"
  executor.cores = "2"
  executor.instances = "2"
  driver.memory = "2g"
  driver.maxResultSize = "1g"
  
  # Shuffle optimization
  sql.shuffle.partitions = "200"
  
  # Master URL for different environments
  master = "local[*]"  # Will be overridden by environment-specific config
  
  # Kubernetes configuration (for cluster mode)
  kubernetes.image = "apache/spark:latest"
  
  # Hive and warehouse settings
  enable-hive = true
  warehouse-dir = "spark-warehouse"
  checkpoint-dir = "checkpoints"
  log-level = "WARN"
}

# =============================================================================
# Data Lake Integration Settings
# =============================================================================
delta.enabled = true
iceberg.enabled = false

# =============================================================================
# Cloud Provider Configuration
# =============================================================================
cloud {
  provider = "aws"  # Options: aws, gcp, azure
}

# AWS Configuration
aws {
  region = "us-east-1"
  s3.bucket = "your-s3-bucket"
  access-key = "your-access-key"
  secret-key = "your-secret-key"
  s3-endpoint = "s3.amazonaws.com"
}

# GCP Configuration
gcp {
  project-id = "your-project-id"
  region = "us-central1"
  storage.bucket = "your-gcs-bucket"
}

# Azure Configuration
azure {
  subscription-id = "your-subscription-id"
  resource-group = "your-resource-group"
  storage.account = "your-storage-account"
}

# =============================================================================
# Data Processing Configuration
# =============================================================================
data {
  input-path = "data/input"
  output-path = "data/output"
  temp-path = "data/temp"
  format = "parquet"
  partition-columns = ["year", "month", "day"]
}

# =============================================================================
# Streaming Configuration
# =============================================================================
streaming {
  kafka {
    bootstrap.servers = "localhost:9092"
    group.id = "spark-streaming-group"
    auto.offset.reset = "latest"
  }
  
  checkpoint.location = "checkpoints/streaming"
  
  # Batch interval for micro-batches
  batch.interval = "10 seconds"
}

# =============================================================================
# Output Path Configuration for Streaming
# =============================================================================
output {
  raw_events_path = "data/raw_events"
  aggregations_path = "data/aggregations"
  sessions_path = "data/user_sessions"
  anomalies_path = "data/anomalies"
}

# =============================================================================
# Application Settings
# =============================================================================
app {
  batch-size = 10000
  processing-mode = "batch"  # Options: batch, streaming
  enable-caching = true
  num-retries = 3
  retry-interval = 5000  # milliseconds
  
  # Performance tuning
  cache-storage-level = "MEMORY_AND_DISK_SER"
  optimization.enabled = true
  
  # Data quality thresholds
  data-quality {
    null-threshold = 0.5
    outlier-threshold = 3.0
  }
}

# =============================================================================
# Machine Learning Configuration
# =============================================================================
ml {
  model-path = "models"
  feature-store-path = "feature-store"
  
  # Default hyperparameters
  random-forest {
    num-trees = 100
    max-depth = 10
    min-instances-per-node = 1
  }
  
  recommendation {
    rank = 50
    max-iter = 20
    reg-param = 0.1
    alpha = 1.0
  }
}

# =============================================================================
# Monitoring and Logging
# =============================================================================
monitoring {
  metrics.enabled = true
  health-check.interval = "30 seconds"
  
  prometheus {
    enabled = true
    port = 9090
  }
  
  grafana {
    enabled = true
    port = 3000
  }
}

# =============================================================================
# Security Configuration
# =============================================================================
security {
  encryption.enabled = false
  kerberos.enabled = false
  ssl.enabled = false
}
