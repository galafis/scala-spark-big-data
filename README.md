# ðŸ‡§ðŸ‡· Big Data com Scala e Apache Spark | ðŸ‡ºðŸ‡¸ Big Data with Scala and Apache Spark

<div align="center">

![Scala](https://img.shields.io/badge/Scala-DC322F?style=for-the-badge&logo=scala&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Hadoop](https://img.shields.io/badge/Apache%20Hadoop-66CCFF?style=for-the-badge&logo=apachehadoop&logoColor=black)
![Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?style=for-the-badge&logo=apachekafka&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD8?style=for-the-badge&logoColor=white)

**Plataforma completa de Big Data com Scala e Apache Spark para processamento distribuÃ­do, streaming em tempo real e analytics avanÃ§ado**

[âš¡ Processamento](#-processamento-distribuÃ­do) â€¢ [ðŸŒŠ Streaming](#-streaming-em-tempo-real) â€¢ [ðŸ“Š Analytics](#-analytics-avanÃ§ado) â€¢ [ðŸš€ Setup](#-setup-rÃ¡pido)

</div>

---

## ðŸ‡§ðŸ‡· PortuguÃªs

### âš¡ VisÃ£o Geral

Plataforma abrangente de **Big Data** desenvolvida com Scala e Apache Spark para processamento distribuÃ­do de grande escala:

- ðŸ”¥ **Apache Spark**: Processamento distribuÃ­do em memÃ³ria para batch e streaming
- ðŸ“Š **Analytics AvanÃ§ado**: Machine Learning, Graph Analytics, SQL distribuÃ­do
- ðŸŒŠ **Streaming Real-time**: Processamento de streams com Kafka e Kinesis
- ðŸ—„ï¸ **Data Lake**: Arquitetura moderna com Delta Lake e Iceberg
- ðŸ”„ **ETL EscalÃ¡vel**: Pipelines de dados distribuÃ­dos e tolerantes a falhas
- ðŸ“ˆ **Monitoramento**: Observabilidade completa com mÃ©tricas e alertas

### ðŸŽ¯ Objetivos da Plataforma

- **Processar petabytes** de dados com performance linear
- **Implementar analytics** em tempo real para decisÃµes instantÃ¢neas
- **Construir data lakes** modernos e governados
- **Automatizar pipelines** de dados complexos e crÃ­ticos
- **Democratizar big data** com APIs simples e documentaÃ§Ã£o clara

### ðŸ› ï¸ Stack TecnolÃ³gico

#### Core Big Data
- **Apache Spark 3.5+**: Engine principal para processamento distribuÃ­do
- **Scala 2.13**: Linguagem principal para desenvolvimento
- **Apache Hadoop**: Sistema de arquivos distribuÃ­do (HDFS)
- **Apache Kafka**: Streaming de dados em tempo real

#### Storage e Data Lake
- **Delta Lake**: TransaÃ§Ãµes ACID em data lakes
- **Apache Iceberg**: Table format para analytics
- **Apache Parquet**: Formato colunar otimizado
- **Apache Avro**: SerializaÃ§Ã£o de dados

#### Cluster e OrquestraÃ§Ã£o
- **Kubernetes**: OrquestraÃ§Ã£o de containers
- **Apache Airflow**: Workflow orchestration
- **YARN**: Resource manager para Hadoop
- **Mesos**: Cluster resource manager

#### Streaming e Messaging
- **Kafka Streams**: Processamento de streams
- **Apache Pulsar**: Sistema de messaging distribuÃ­do
- **Amazon Kinesis**: Streaming na AWS
- **Apache Flink**: Stream processing alternativo

#### Machine Learning e Analytics
- **Spark MLlib**: Machine learning distribuÃ­do
- **GraphX**: Processamento de grafos
- **Spark SQL**: SQL distribuÃ­do
- **Apache Zeppelin**: Notebooks interativos

#### Monitoramento e Observabilidade
- **Prometheus**: Coleta de mÃ©tricas
- **Grafana**: VisualizaÃ§Ã£o de mÃ©tricas
- **ELK Stack**: Logs centralizados
- **Jaeger**: Distributed tracing

### ðŸ“‹ Estrutura da Plataforma

```
scala-spark-big-data/
â”œâ”€â”€ ðŸ“ src/                        # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ðŸ“ main/                   # CÃ³digo principal
â”‚   â”‚   â”œâ”€â”€ ðŸ“ scala/              # CÃ³digo Scala
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ com/galafis/bigdata/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ core/       # Componentes core
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ SparkSessionManager.scala # Gerenciador de sessÃµes
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ ConfigManager.scala # Gerenciamento de configuraÃ§Ã£o
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DataFrameUtils.scala # UtilitÃ¡rios DataFrame
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ PerformanceMonitor.scala # Monitor de performance
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ etl/        # ETL pipelines
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DataIngestion.scala # IngestÃ£o de dados
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DataTransformation.scala # TransformaÃ§Ãµes
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DataValidation.scala # ValidaÃ§Ã£o de dados
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DataQuality.scala # Qualidade de dados
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ DataLineage.scala # Linhagem de dados
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ streaming/  # Streaming real-time
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ KafkaStreaming.scala # Streaming Kafka
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ KinesisStreaming.scala # Streaming Kinesis
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ StreamProcessor.scala # Processador de streams
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ WindowOperations.scala # OperaÃ§Ãµes de janela
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ StateManagement.scala # Gerenciamento de estado
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ analytics/  # Analytics avanÃ§ado
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ MLPipelines.scala # Pipelines ML
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ GraphAnalytics.scala # Analytics de grafos
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ TimeSeriesAnalysis.scala # AnÃ¡lise temporal
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ StatisticalAnalysis.scala # AnÃ¡lise estatÃ­stica
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ RecommendationEngine.scala # Motor de recomendaÃ§Ã£o
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ storage/    # Camada de storage
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DeltaLakeManager.scala # Gerenciador Delta Lake
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ IcebergManager.scala # Gerenciador Iceberg
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ HiveMetastore.scala # IntegraÃ§Ã£o Hive
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ S3Manager.scala # Gerenciador S3
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ HDFSManager.scala # Gerenciador HDFS
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ optimization/ # OtimizaÃ§Ãµes
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ CostBasedOptimizer.scala # Otimizador baseado em custo
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ PartitionOptimizer.scala # Otimizador de partiÃ§Ãµes
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ CacheManager.scala # Gerenciador de cache
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ ResourceOptimizer.scala # Otimizador de recursos
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ security/   # SeguranÃ§a
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ KerberosAuth.scala # AutenticaÃ§Ã£o Kerberos
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ DataEncryption.scala # Criptografia de dados
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ AccessControl.scala # Controle de acesso
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ AuditLogger.scala # Logger de auditoria
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ monitoring/ # Monitoramento
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ MetricsCollector.scala # Coletor de mÃ©tricas
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ AlertManager.scala # Gerenciador de alertas
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ HealthChecker.scala # Verificador de saÃºde
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ PerformanceProfiler.scala # Profiler de performance
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“ utils/      # UtilitÃ¡rios
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ DateTimeUtils.scala # UtilitÃ¡rios de data/hora
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ StringUtils.scala # UtilitÃ¡rios de string
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ JsonUtils.scala # UtilitÃ¡rios JSON
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ FileUtils.scala # UtilitÃ¡rios de arquivo
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ ðŸ“„ NetworkUtils.scala # UtilitÃ¡rios de rede
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“ apps/           # AplicaÃ§Ãµes especÃ­ficas
â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ DataLakeBuilder.scala # Construtor de data lake
â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ RealtimeAnalytics.scala # Analytics tempo real
â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ BatchProcessor.scala # Processador batch
â”‚   â”‚   â”‚       â”œâ”€â”€ ðŸ“„ MLTrainingPipeline.scala # Pipeline treinamento ML
â”‚   â”‚   â”‚       â””â”€â”€ ðŸ“„ DataMigration.scala # MigraÃ§Ã£o de dados
â”‚   â”‚   â””â”€â”€ ðŸ“ resources/          # Recursos
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ application.conf # ConfiguraÃ§Ã£o principal
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ spark-defaults.conf # ConfiguraÃ§Ãµes Spark
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ log4j2.xml      # ConfiguraÃ§Ã£o de logs
â”‚   â”‚       â””â”€â”€ ðŸ“„ reference.conf  # ConfiguraÃ§Ãµes de referÃªncia
â”‚   â””â”€â”€ ðŸ“ test/                   # Testes
â”‚       â”œâ”€â”€ ðŸ“ scala/              # Testes Scala
â”‚       â”‚   â”œâ”€â”€ ðŸ“ unit/           # Testes unitÃ¡rios
â”‚       â”‚   â”œâ”€â”€ ðŸ“ integration/    # Testes de integraÃ§Ã£o
â”‚       â”‚   â””â”€â”€ ðŸ“ performance/    # Testes de performance
â”‚       â””â”€â”€ ðŸ“ resources/          # Recursos de teste
â”‚           â”œâ”€â”€ ðŸ“„ test-data/      # Dados de teste
â”‚           â””â”€â”€ ðŸ“„ test.conf       # ConfiguraÃ§Ãµes de teste
â”œâ”€â”€ ðŸ“ scripts/                    # Scripts de deployment
â”‚   â”œâ”€â”€ ðŸ“ deployment/             # Scripts de deploy
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ deploy-cluster.sh   # Deploy do cluster
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ setup-kafka.sh      # Setup Kafka
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ setup-hadoop.sh     # Setup Hadoop
â”‚   â”‚   â””â”€â”€ ðŸ“„ setup-monitoring.sh # Setup monitoramento
â”‚   â”œâ”€â”€ ðŸ“ data-generation/        # GeraÃ§Ã£o de dados
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ generate-sample-data.scala # Gerar dados de exemplo
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ kafka-producer.scala # Produtor Kafka
â”‚   â”‚   â””â”€â”€ ðŸ“„ data-simulator.scala # Simulador de dados
â”‚   â”œâ”€â”€ ðŸ“ maintenance/            # ManutenÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ cleanup-old-data.sh # Limpeza de dados antigos
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ optimize-tables.sh  # OtimizaÃ§Ã£o de tabelas
â”‚   â”‚   â””â”€â”€ ðŸ“„ backup-metadata.sh  # Backup de metadados
â”‚   â””â”€â”€ ðŸ“ monitoring/             # Scripts de monitoramento
â”‚       â”œâ”€â”€ ðŸ“„ health-check.sh     # VerificaÃ§Ã£o de saÃºde
â”‚       â”œâ”€â”€ ðŸ“„ performance-report.sh # RelatÃ³rio de performance
â”‚       â””â”€â”€ ðŸ“„ alert-setup.sh      # Setup de alertas
â”œâ”€â”€ ðŸ“ docker/                     # Containers Docker
â”‚   â”œâ”€â”€ ðŸ“„ Dockerfile.spark        # Container Spark
â”‚   â”œâ”€â”€ ðŸ“„ Dockerfile.kafka        # Container Kafka
â”‚   â”œâ”€â”€ ðŸ“„ Dockerfile.hadoop       # Container Hadoop
â”‚   â”œâ”€â”€ ðŸ“„ docker-compose.yml      # OrquestraÃ§Ã£o local
â”‚   â””â”€â”€ ðŸ“„ docker-compose.prod.yml # OrquestraÃ§Ã£o produÃ§Ã£o
â”œâ”€â”€ ðŸ“ kubernetes/                 # Manifests Kubernetes
â”‚   â”œâ”€â”€ ðŸ“ spark/                  # Manifests Spark
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ spark-master.yaml   # Spark master
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ spark-worker.yaml   # Spark workers
â”‚   â”‚   â””â”€â”€ ðŸ“„ spark-history.yaml  # Spark history server
â”‚   â”œâ”€â”€ ðŸ“ kafka/                  # Manifests Kafka
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ kafka-cluster.yaml  # Cluster Kafka
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ zookeeper.yaml      # Zookeeper
â”‚   â”‚   â””â”€â”€ ðŸ“„ kafka-connect.yaml  # Kafka Connect
â”‚   â”œâ”€â”€ ðŸ“ monitoring/             # Manifests monitoramento
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ prometheus.yaml     # Prometheus
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ grafana.yaml        # Grafana
â”‚   â”‚   â””â”€â”€ ðŸ“„ alertmanager.yaml   # Alert Manager
â”‚   â””â”€â”€ ðŸ“ storage/                # Manifests storage
â”‚       â”œâ”€â”€ ðŸ“„ hdfs-namenode.yaml  # HDFS NameNode
â”‚       â”œâ”€â”€ ðŸ“„ hdfs-datanode.yaml  # HDFS DataNode
â”‚       â””â”€â”€ ðŸ“„ minio.yaml          # MinIO (S3 compatible)
â”œâ”€â”€ ðŸ“ notebooks/                  # Jupyter/Zeppelin notebooks
â”‚   â”œâ”€â”€ ðŸ“„ 01_data_exploration.ipynb # ExploraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ ðŸ“„ 02_etl_pipeline.ipynb   # Pipeline ETL
â”‚   â”œâ”€â”€ ðŸ“„ 03_streaming_analytics.ipynb # Analytics streaming
â”‚   â”œâ”€â”€ ðŸ“„ 04_machine_learning.ipynb # Machine learning
â”‚   â”œâ”€â”€ ðŸ“„ 05_graph_analytics.ipynb # Analytics de grafos
â”‚   â””â”€â”€ ðŸ“„ 06_performance_tuning.ipynb # Tuning de performance
â”œâ”€â”€ ðŸ“ data/                       # Dados de exemplo
â”‚   â”œâ”€â”€ ðŸ“ sample/                 # Dados de exemplo
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ sales_data.parquet  # Dados de vendas
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ customer_data.json  # Dados de clientes
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ product_catalog.csv # CatÃ¡logo de produtos
â”‚   â”‚   â””â”€â”€ ðŸ“„ transaction_log.avro # Log de transaÃ§Ãµes
â”‚   â”œâ”€â”€ ðŸ“ schemas/                # Esquemas de dados
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ sales_schema.json   # Schema vendas
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ customer_schema.avsc # Schema clientes
â”‚   â”‚   â””â”€â”€ ðŸ“„ product_schema.sql  # Schema produtos
â”‚   â””â”€â”€ ðŸ“ reference/              # Dados de referÃªncia
â”‚       â”œâ”€â”€ ðŸ“„ country_codes.csv   # CÃ³digos de paÃ­s
â”‚       â”œâ”€â”€ ðŸ“„ currency_rates.json # Taxas de cÃ¢mbio
â”‚       â””â”€â”€ ðŸ“„ time_zones.parquet  # Fusos horÃ¡rios
â”œâ”€â”€ ðŸ“ docs/                       # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ ðŸ“ architecture/           # DocumentaÃ§Ã£o de arquitetura
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ system-design.md    # Design do sistema
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ data-flow.md        # Fluxo de dados
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ scalability.md      # Escalabilidade
â”‚   â”‚   â””â”€â”€ ðŸ“„ security.md         # SeguranÃ§a
â”‚   â”œâ”€â”€ ðŸ“ deployment/             # DocumentaÃ§Ã£o de deploy
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ cluster-setup.md    # Setup do cluster
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ configuration.md    # ConfiguraÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ monitoring.md       # Monitoramento
â”‚   â”‚   â””â”€â”€ ðŸ“„ troubleshooting.md  # SoluÃ§Ã£o de problemas
â”‚   â”œâ”€â”€ ðŸ“ development/            # DocumentaÃ§Ã£o de desenvolvimento
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ coding-standards.md # PadrÃµes de cÃ³digo
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ testing.md          # Testes
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ performance.md      # Performance
â”‚   â”‚   â””â”€â”€ ðŸ“„ best-practices.md   # Melhores prÃ¡ticas
â”‚   â””â”€â”€ ðŸ“ api/                    # DocumentaÃ§Ã£o da API
â”‚       â”œâ”€â”€ ðŸ“„ core-api.md         # API core
â”‚       â”œâ”€â”€ ðŸ“„ streaming-api.md    # API streaming
â”‚       â””â”€â”€ ðŸ“„ analytics-api.md    # API analytics
â”œâ”€â”€ ðŸ“ terraform/                  # Infrastructure as Code
â”‚   â”œâ”€â”€ ðŸ“ aws/                    # Infraestrutura AWS
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ main.tf             # ConfiguraÃ§Ã£o principal
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emr-cluster.tf      # Cluster EMR
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ s3-buckets.tf       # Buckets S3
â”‚   â”‚   â””â”€â”€ ðŸ“„ kinesis-streams.tf  # Streams Kinesis
â”‚   â”œâ”€â”€ ðŸ“ gcp/                    # Infraestrutura GCP
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ main.tf             # ConfiguraÃ§Ã£o principal
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ dataproc-cluster.tf # Cluster Dataproc
â”‚   â”‚   â””â”€â”€ ðŸ“„ bigquery-datasets.tf # Datasets BigQuery
â”‚   â””â”€â”€ ðŸ“ azure/                  # Infraestrutura Azure
â”‚       â”œâ”€â”€ ðŸ“„ main.tf             # ConfiguraÃ§Ã£o principal
â”‚       â”œâ”€â”€ ðŸ“„ hdinsight-cluster.tf # Cluster HDInsight
â”‚       â””â”€â”€ ðŸ“„ storage-accounts.tf # Contas de storage
â”œâ”€â”€ ðŸ“„ build.sbt                   # Build configuration
â”œâ”€â”€ ðŸ“„ project/                    # ConfiguraÃ§Ã£o do projeto
â”‚   â”œâ”€â”€ ðŸ“„ build.properties        # VersÃ£o do SBT
â”‚   â”œâ”€â”€ ðŸ“„ plugins.sbt             # Plugins SBT
â”‚   â””â”€â”€ ðŸ“„ Dependencies.scala      # DependÃªncias
â”œâ”€â”€ ðŸ“„ README.md                   # Este arquivo
â”œâ”€â”€ ðŸ“„ LICENSE                     # LicenÃ§a Apache 2.0
â””â”€â”€ ðŸ“„ .gitignore                 # Arquivos ignorados
```

### âš¡ Processamento DistribuÃ­do

#### 1. ðŸ”¥ Core Spark Framework

**Gerenciador de SessÃµes Spark Otimizado**
```scala
package com.galafis.bigdata.core

import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.sql.catalyst.expressions.codegen.CodegenObjectFactoryMode
import com.typesafe.config.{Config, ConfigFactory}
import scala.util.{Try, Success, Failure}

/**
 * Gerenciador centralizado de sessÃµes Spark com otimizaÃ§Ãµes avanÃ§adas
 */
object SparkSessionManager {
  
  private var sparkSession: Option[SparkSession] = None
  private val config: Config = ConfigFactory.load()
  
  /**
   * Criar ou obter sessÃ£o Spark otimizada
   */
  def getOrCreateSession(appName: String = "BigDataPlatform"): SparkSession = {
    sparkSession.getOrElse {
      val session = createOptimizedSession(appName)
      sparkSession = Some(session)
      
      // Configurar hooks de shutdown
      sys.addShutdownHook {
        stopSession()
      }
      
      session
    }
  }
  
  /**
   * Criar sessÃ£o Spark com configuraÃ§Ãµes otimizadas
   */
  private def createOptimizedSession(appName: String): SparkSession = {
    val conf = new SparkConf()
      .setAppName(appName)
      .set("spark.serializer", classOf[KryoSerializer].getName)
      .set("spark.sql.adaptive.enabled", "true")
      .set("spark.sql.adaptive.coalescePartitions.enabled", "true")
      .set("spark.sql.adaptive.skewJoin.enabled", "true")
      .set("spark.sql.codegen.wholeStage", "true")
      .set("spark.sql.codegen.factoryMode", CodegenObjectFactoryMode.CODEGEN_ONLY.toString)
      
    // ConfiguraÃ§Ãµes de memÃ³ria otimizadas
    conf.set("spark.executor.memory", config.getString("spark.executor.memory"))
        .set("spark.executor.cores", config.getString("spark.executor.cores"))
        .set("spark.executor.instances", config.getString("spark.executor.instances"))
        .set("spark.driver.memory", config.getString("spark.driver.memory"))
        .set("spark.driver.maxResultSize", config.getString("spark.driver.maxResultSize"))
        
    // ConfiguraÃ§Ãµes de shuffle otimizadas
    conf.set("spark.sql.shuffle.partitions", config.getString("spark.sql.shuffle.partitions"))
        .set("spark.shuffle.service.enabled", "true")
        .set("spark.shuffle.compress", "true")
        .set("spark.shuffle.spill.compress", "true")
        
    // ConfiguraÃ§Ãµes de cache
    conf.set("spark.sql.inMemoryColumnarStorage.compressed", "true")
        .set("spark.sql.inMemoryColumnarStorage.batchSize", "20000")
        
    // ConfiguraÃ§Ãµes de Delta Lake se habilitado
    if (config.getBoolean("delta.enabled")) {
      conf.set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
          .set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    }
    
    // ConfiguraÃ§Ãµes de Iceberg se habilitado
    if (config.getBoolean("iceberg.enabled")) {
      conf.set("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
          .set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
          .set("spark.sql.catalog.spark_catalog.type", "hive")
    }
    
    // ConfiguraÃ§Ãµes especÃ­ficas do ambiente
    val environment = config.getString("environment")
    environment match {
      case "local" => configureLocalMode(conf)
      case "cluster" => configureClusterMode(conf)
      case "cloud" => configureCloudMode(conf)
    }
    
    SparkSession.builder()
      .config(conf)
      .enableHiveSupport()
      .getOrCreate()
  }
  
  /**
   * ConfiguraÃ§Ãµes para modo local
   */
  private def configureLocalMode(conf: SparkConf): Unit = {
    conf.setMaster("local[*]")
        .set("spark.sql.warehouse.dir", "/tmp/spark-warehouse")
        .set("spark.driver.bindAddress", "127.0.0.1")
  }
  
  /**
   * ConfiguraÃ§Ãµes para modo cluster
   */
  private def configureClusterMode(conf: SparkConf): Unit = {
    conf.setMaster(config.getString("spark.master"))
        .set("spark.submit.deployMode", "cluster")
        .set("spark.kubernetes.container.image", config.getString("spark.kubernetes.image"))
        .set("spark.kubernetes.authenticate.driver.serviceAccountName", "spark")
  }
  
  /**
   * ConfiguraÃ§Ãµes para modo cloud
   */
  private def configureCloudMode(conf: SparkConf): Unit = {
    val cloudProvider = config.getString("cloud.provider")
    
    cloudProvider match {
      case "aws" => configureAWS(conf)
      case "gcp" => configureGCP(conf)
      case "azure" => configureAzure(conf)
    }
  }
  
  /**
   * ConfiguraÃ§Ãµes especÃ­ficas para AWS
   */
  private def configureAWS(conf: SparkConf): Unit = {
    conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .set("spark.hadoop.fs.s3a.aws.credentials.provider", 
             "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")
        .set("spark.hadoop.fs.s3a.fast.upload", "true")
        .set("spark.hadoop.fs.s3a.block.size", "134217728") // 128MB
        .set("spark.hadoop.fs.s3a.multipart.size", "67108864") // 64MB
  }
  
  /**
   * ConfiguraÃ§Ãµes especÃ­ficas para GCP
   */
  private def configureGCP(conf: SparkConf): Unit = {
    conf.set("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
        .set("spark.hadoop.fs.AbstractFileSystem.gs.impl", 
             "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
        .set("spark.hadoop.google.cloud.auth.service.account.enable", "true")
  }
  
  /**
   * ConfiguraÃ§Ãµes especÃ­ficas para Azure
   */
  private def configureAzure(conf: SparkConf): Unit = {
    conf.set("spark.hadoop.fs.azure.account.auth.type", "OAuth")
        .set("spark.hadoop.fs.azure.account.oauth.provider.type", 
             "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
  }
  
  /**
   * Parar sessÃ£o Spark
   */
  def stopSession(): Unit = {
    sparkSession.foreach { session =>
      Try {
        session.stop()
      } match {
        case Success(_) => println("Spark session stopped successfully")
        case Failure(exception) => println(s"Error stopping Spark session: ${exception.getMessage}")
      }
    }
    sparkSession = None
  }
  
  /**
   * Obter sessÃ£o atual
   */
  def getCurrentSession: Option[SparkSession] = sparkSession
  
  /**
   * Verificar se sessÃ£o estÃ¡ ativa
   */
  def isSessionActive: Boolean = sparkSession.exists(!_.sparkContext.isStopped)
  
  /**
   * Recriar sessÃ£o com novas configuraÃ§Ãµes
   */
  def recreateSession(appName: String = "BigDataPlatform"): SparkSession = {
    stopSession()
    getOrCreateSession(appName)
  }
}

/**
 * UtilitÃ¡rios para DataFrames com otimizaÃ§Ãµes avanÃ§adas
 */
object DataFrameUtils {
  
  import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
  import org.apache.spark.sql.functions._
  import org.apache.spark.sql.types._
  import org.apache.spark.storage.StorageLevel
  
  /**
   * Cache inteligente baseado no tamanho dos dados
   */
  def smartCache(df: DataFrame): DataFrame = {
    val spark = df.sparkSession
    
    // Estimar tamanho do DataFrame
    val estimatedSize = estimateDataFrameSize(df)
    val availableMemory = getAvailableMemory(spark)
    
    if (estimatedSize < availableMemory * 0.3) {
      // Cache em memÃ³ria se cabe confortavelmente
      df.cache()
    } else if (estimatedSize < availableMemory * 0.8) {
      // Cache com serializaÃ§Ã£o se Ã© grande mas ainda cabe
      df.persist(StorageLevel.MEMORY_AND_DISK_SER)
    } else {
      // Apenas em disco se Ã© muito grande
      df.persist(StorageLevel.DISK_ONLY)
    }
  }
  
  /**
   * Estimar tamanho de um DataFrame
   */
  private def estimateDataFrameSize(df: DataFrame): Long = {
    val numRows = df.count()
    val avgRowSize = df.schema.fields.map(estimateFieldSize).sum
    numRows * avgRowSize
  }
  
  /**
   * Estimar tamanho de um campo
   */
  private def estimateFieldSize(field: StructField): Long = {
    field.dataType match {
      case IntegerType => 4
      case LongType => 8
      case DoubleType => 8
      case FloatType => 4
      case BooleanType => 1
      case StringType => 50 // Estimativa mÃ©dia
      case TimestampType => 8
      case DateType => 4
      case _ => 20 // Estimativa para tipos complexos
    }
  }
  
  /**
   * Obter memÃ³ria disponÃ­vel
   */
  private def getAvailableMemory(spark: SparkSession): Long = {
    val executorMemory = spark.conf.get("spark.executor.memory", "1g")
    val numExecutors = spark.conf.get("spark.executor.instances", "1").toInt
    
    // Converter string de memÃ³ria para bytes (simplificado)
    val memoryInGB = executorMemory.replace("g", "").replace("G", "").toDouble
    (memoryInGB * 1024 * 1024 * 1024 * numExecutors * 0.6).toLong // 60% da memÃ³ria disponÃ­vel
  }
  
  /**
   * Otimizar particionamento baseado no tamanho dos dados
   */
  def optimizePartitioning(df: DataFrame, targetPartitionSize: Long = 128 * 1024 * 1024): DataFrame = {
    val currentSize = estimateDataFrameSize(df)
    val optimalPartitions = Math.max(1, (currentSize / targetPartitionSize).toInt)
    
    if (df.rdd.getNumPartitions > optimalPartitions * 2) {
      // Muitas partiÃ§Ãµes pequenas - coalescer
      df.coalesce(optimalPartitions)
    } else if (df.rdd.getNumPartitions < optimalPartitions / 2) {
      // Poucas partiÃ§Ãµes grandes - reparticionar
      df.repartition(optimalPartitions)
    } else {
      // Particionamento jÃ¡ estÃ¡ bom
      df
    }
  }
  
  /**
   * AnÃ¡lise de qualidade de dados
   */
  def analyzeDataQuality(df: DataFrame): Map[String, Any] = {
    val totalRows = df.count()
    
    val qualityMetrics = df.schema.fields.map { field =>
      val colName = field.name
      val nullCount = df.filter(col(colName).isNull).count()
      val nullPercentage = (nullCount.toDouble / totalRows) * 100
      
      val distinctCount = df.select(colName).distinct().count()
      val uniquenessPercentage = (distinctCount.toDouble / totalRows) * 100
      
      colName -> Map(
        "null_count" -> nullCount,
        "null_percentage" -> nullPercentage,
        "distinct_count" -> distinctCount,
        "uniqueness_percentage" -> uniquenessPercentage
      )
    }.toMap
    
    Map(
      "total_rows" -> totalRows,
      "column_metrics" -> qualityMetrics
    )
  }
  
  /**
   * Detectar e tratar outliers usando IQR
   */
  def detectOutliers(df: DataFrame, numericColumns: Seq[String]): DataFrame = {
    var result = df
    
    numericColumns.foreach { colName =>
      val quantiles = df.stat.approxQuantile(colName, Array(0.25, 0.75), 0.05)
      val q1 = quantiles(0)
      val q3 = quantiles(1)
      val iqr = q3 - q1
      val lowerBound = q1 - 1.5 * iqr
      val upperBound = q3 + 1.5 * iqr
      
      result = result.withColumn(
        s"${colName}_outlier",
        when(col(colName) < lowerBound || col(colName) > upperBound, true).otherwise(false)
      )
    }
    
    result
  }
}
```

#### 2. ðŸŒŠ Streaming em Tempo Real

**Processador de Streams Kafka AvanÃ§ado**
```scala
package com.galafis.bigdata.streaming

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import com.galafis.bigdata.core.SparkSessionManager
import com.typesafe.config.ConfigFactory
import scala.concurrent.duration._

/**
 * Processador avanÃ§ado de streams Kafka com gerenciamento de estado e tolerÃ¢ncia a falhas
 */
class KafkaStreamProcessor(
  brokers: String,
  topics: Seq[String],
  checkpointLocation: String
) {
  
  private val spark: SparkSession = SparkSessionManager.getOrCreateSession("KafkaStreamProcessor")
  private val config = ConfigFactory.load()
  
  import spark.implicits._
  
  /**
   * Schema para mensagens Kafka
   */
  private val messageSchema = StructType(Seq(
    StructField("timestamp", TimestampType, nullable = false),
    StructField("user_id", StringType, nullable = false),
    StructField("event_type", StringType, nullable = false),
    StructField("properties", MapType(StringType, StringType), nullable = true),
    StructField("session_id", StringType, nullable = true),
    StructField("device_type", StringType, nullable = true)
  ))
  
  /**
   * Criar stream reader para Kafka
   */
  def createKafkaStream(): DataFrame = {
    spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", brokers)
      .option("subscribe", topics.mkString(","))
      .option("startingOffsets", "latest")
      .option("failOnDataLoss", "false")
      .option("kafka.session.timeout.ms", "30000")
      .option("kafka.request.timeout.ms", "40000")
      .option("kafka.max.poll.records", "1000")
      .option("kafka.fetch.max.wait.ms", "500")
      .load()
  }
  
  /**
   * Processar mensagens com parsing e validaÃ§Ã£o
   */
  def processMessages(rawStream: DataFrame): DataFrame = {
    rawStream
      .select(
        col("key").cast("string").as("message_key"),
        col("value").cast("string").as("message_value"),
        col("topic"),
        col("partition"),
        col("offset"),
        col("timestamp").as("kafka_timestamp")
      )
      .filter(col("message_value").isNotNull)
      .withColumn("parsed_message", from_json(col("message_value"), messageSchema))
      .select(
        col("message_key"),
        col("topic"),
        col("partition"),
        col("offset"),
        col("kafka_timestamp"),
        col("parsed_message.*")
      )
      .filter(col("timestamp").isNotNull && col("user_id").isNotNull)
  }
  
  /**
   * AgregaÃ§Ãµes em janelas de tempo
   */
  def createWindowedAggregations(processedStream: DataFrame): DataFrame = {
    processedStream
      .withWatermark("timestamp", "10 minutes")
      .groupBy(
        window(col("timestamp"), "5 minutes", "1 minute"),
        col("event_type"),
        col("device_type")
      )
      .agg(
        count("*").as("event_count"),
        countDistinct("user_id").as("unique_users"),
        countDistinct("session_id").as("unique_sessions"),
        avg(size(col("properties"))).as("avg_properties_count")
      )
      .select(
        col("window.start").as("window_start"),
        col("window.end").as("window_end"),
        col("event_type"),
        col("device_type"),
        col("event_count"),
        col("unique_users"),
        col("unique_sessions"),
        col("avg_properties_count")
      )
  }
  
  /**
   * DetecÃ§Ã£o de anomalias em tempo real
   */
  def detectAnomalies(aggregatedStream: DataFrame): DataFrame = {
    // Usar estatÃ­sticas mÃ³veis para detectar anomalias
    aggregatedStream
      .withColumn("event_count_zscore", 
        (col("event_count") - avg("event_count").over(
          org.apache.spark.sql.expressions.Window
            .partitionBy("event_type", "device_type")
            .orderBy("window_start")
            .rowsBetween(-10, 0)
        )) / stddev("event_count").over(
          org.apache.spark.sql.expressions.Window
            .partitionBy("event_type", "device_type")
            .orderBy("window_start")
            .rowsBetween(-10, 0)
        )
      )
      .withColumn("is_anomaly", 
        when(abs(col("event_count_zscore")) > 3.0, true).otherwise(false)
      )
      .filter(col("is_anomaly") === true)
  }
  
  /**
   * Processamento de sessÃµes de usuÃ¡rio
   */
  def processUserSessions(processedStream: DataFrame): DataFrame = {
    processedStream
      .withWatermark("timestamp", "30 minutes")
      .groupBy(
        col("user_id"),
        col("session_id"),
        window(col("timestamp"), "30 minutes")
      )
      .agg(
        min("timestamp").as("session_start"),
        max("timestamp").as("session_end"),
        count("*").as("events_in_session"),
        countDistinct("event_type").as("unique_event_types"),
        collect_list("event_type").as("event_sequence")
      )
      .withColumn("session_duration_minutes",
        (unix_timestamp(col("session_end")) - unix_timestamp(col("session_start"))) / 60
      )
      .filter(col("session_duration_minutes") > 0)
  }
  
  /**
   * Sink para Delta Lake com particionamento otimizado
   */
  def writeToDeltaLake(
    stream: DataFrame,
    outputPath: String,
    partitionColumns: Seq[String] = Seq("event_type")
  ): StreamingQuery = {
    
    val writer = stream.writeStream
      .format("delta")
      .outputMode("append")
      .option("checkpointLocation", s"$checkpointLocation/delta")
      .option("path", outputPath)
      .trigger(Trigger.ProcessingTime(30.seconds))
    
    if (partitionColumns.nonEmpty) {
      writer.partitionBy(partitionColumns: _*)
    }
    
    writer.start()
  }
  
  /**
   * Sink para Kafka (para downstream processing)
   */
  def writeToKafka(
    stream: DataFrame,
    outputTopic: String,
    keyColumn: String = "user_id"
  ): StreamingQuery = {
    
    stream
      .select(
        col(keyColumn).as("key"),
        to_json(struct(col("*"))).as("value")
      )
      .writeStream
      .format("kafka")
      .option("kafka.bootstrap.servers", brokers)
      .option("topic", outputTopic)
      .option("checkpointLocation", s"$checkpointLocation/kafka")
      .trigger(Trigger.ProcessingTime(10.seconds))
      .start()
  }
  
  /**
   * Sink para console (para debugging)
   */
  def writeToConsole(stream: DataFrame, numRows: Int = 20): StreamingQuery = {
    stream.writeStream
      .format("console")
      .option("numRows", numRows)
      .option("truncate", false)
      .trigger(Trigger.ProcessingTime(30.seconds))
      .start()
  }
  
  /**
   * Pipeline completo de processamento
   */
  def runCompleteProcessingPipeline(): Map[String, StreamingQuery] = {
    // Stream principal
    val rawStream = createKafkaStream()
    val processedStream = processMessages(rawStream)
    
    // AgregaÃ§Ãµes em janelas
    val windowedAggregations = createWindowedAggregations(processedStream)
    
    // DetecÃ§Ã£o de anomalias
    val anomalies = detectAnomalies(windowedAggregations)
    
    // SessÃµes de usuÃ¡rio
    val userSessions = processUserSessions(processedStream)
    
    // MÃºltiplos sinks
    val queries = Map(
      "raw_events" -> writeToDeltaLake(
        processedStream, 
        config.getString("output.raw_events_path"),
        Seq("event_type", "device_type")
      ),
      "aggregations" -> writeToDeltaLake(
        windowedAggregations,
        config.getString("output.aggregations_path"),
        Seq("event_type")
      ),
      "anomalies" -> writeToKafka(anomalies, "anomaly-alerts", "event_type"),
      "user_sessions" -> writeToDeltaLake(
        userSessions,
        config.getString("output.sessions_path"),
        Seq("user_id")
      )
    )
    
    // Monitoramento
    queries.foreach { case (name, query) =>
      println(s"Started streaming query: $name")
      query.awaitTermination(1000) // Check status
    }
    
    queries
  }
  
  /**
   * Parar todas as queries
   */
  def stopAllQueries(queries: Map[String, StreamingQuery]): Unit = {
    queries.foreach { case (name, query) =>
      if (query.isActive) {
        println(s"Stopping query: $name")
        query.stop()
      }
    }
  }
}

/**
 * Gerenciamento de estado para streams
 */
object StateManagement {
  
  import org.apache.spark.sql.streaming.GroupState
  import org.apache.spark.sql.streaming.GroupStateTimeout
  
  /**
   * Estado de sessÃ£o de usuÃ¡rio
   */
  case class UserSessionState(
    userId: String,
    sessionId: String,
    startTime: Long,
    lastEventTime: Long,
    eventCount: Int,
    eventTypes: Set[String]
  )
  
  /**
   * Evento de entrada
   */
  case class UserEvent(
    userId: String,
    sessionId: String,
    eventType: String,
    timestamp: Long,
    properties: Map[String, String]
  )
  
  /**
   * FunÃ§Ã£o de atualizaÃ§Ã£o de estado para sessÃµes
   */
  def updateUserSessionState(
    key: String,
    events: Iterator[UserEvent],
    state: GroupState[UserSessionState]
  ): Iterator[UserSessionState] = {
    
    val currentState = state.getOption.getOrElse(
      UserSessionState(key, "", 0L, 0L, 0, Set.empty)
    )
    
    val eventsList = events.toList
    if (eventsList.isEmpty) {
      // Timeout - retornar estado final
      state.remove()
      Iterator(currentState)
    } else {
      // Atualizar estado com novos eventos
      val latestEvent = eventsList.maxBy(_.timestamp)
      val allEventTypes = currentState.eventTypes ++ eventsList.map(_.eventType).toSet
      
      val newState = currentState.copy(
        sessionId = latestEvent.sessionId,
        startTime = if (currentState.startTime == 0L) latestEvent.timestamp else currentState.startTime,
        lastEventTime = latestEvent.timestamp,
        eventCount = currentState.eventCount + eventsList.size,
        eventTypes = allEventTypes
      )
      
      // Configurar timeout para 30 minutos de inatividade
      state.setTimeoutDuration("30 minutes")
      state.update(newState)
      
      Iterator(newState)
    }
  }
}
```

#### 3. ðŸ“Š Analytics AvanÃ§ado

**Engine de Machine Learning DistribuÃ­do**
```scala
package com.galafis.bigdata.analytics

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.feature._
import org.apache.spark.ml.classification._
import org.apache.spark.ml.regression._
import org.apache.spark.ml.clustering._
import org.apache.spark.ml.recommendation._
import org.apache.spark.ml.evaluation._
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import com.galafis.bigdata.core.SparkSessionManager
import scala.util.{Try, Success, Failure}

/**
 * Engine de Machine Learning distribuÃ­do com pipelines automatizados
 */
class MLPipelineEngine(spark: SparkSession = SparkSessionManager.getOrCreateSession()) {
  
  import spark.implicits._
  
  /**
   * Pipeline de classificaÃ§Ã£o automatizado
   */
  def createClassificationPipeline(
    data: DataFrame,
    labelCol: String,
    featureCols: Seq[String],
    algorithmType: String = "randomforest"
  ): (PipelineModel, Map[String, Double]) = {
    
    // PreparaÃ§Ã£o de features
    val featurePreprocessor = createFeaturePreprocessor(data, featureCols)
    
    // Seletor de algoritmo
    val classifier = algorithmType.toLowerCase match {
      case "randomforest" => new RandomForestClassifier()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setNumTrees(100)
        .setMaxDepth(10)
        .setSubsamplingRate(0.8)
        
      case "gradientboosting" => new GBTClassifier()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setMaxIter(100)
        .setMaxDepth(8)
        .setStepSize(0.1)
        
      case "logisticregression" => new LogisticRegression()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setMaxIter(100)
        .setRegParam(0.01)
        .setElasticNetParam(0.5)
        
      case "naivebayes" => new NaiveBayes()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setSmoothing(1.0)
        
      case _ => throw new IllegalArgumentException(s"Unsupported algorithm: $algorithmType")
    }
    
    // Pipeline completo
    val pipeline = new Pipeline()
      .setStages(featurePreprocessor :+ classifier)
    
    // Split treino/teste
    val Array(trainData, testData) = data.randomSplit(Array(0.8, 0.2), seed = 42)
    
    // Treinar modelo
    val model = pipeline.fit(trainData)
    
    // Avaliar modelo
    val predictions = model.transform(testData)
    val metrics = evaluateClassificationModel(predictions, labelCol)
    
    (model, metrics)
  }
  
  /**
   * Pipeline de regressÃ£o automatizado
   */
  def createRegressionPipeline(
    data: DataFrame,
    labelCol: String,
    featureCols: Seq[String],
    algorithmType: String = "randomforest"
  ): (PipelineModel, Map[String, Double]) = {
    
    val featurePreprocessor = createFeaturePreprocessor(data, featureCols)
    
    val regressor = algorithmType.toLowerCase match {
      case "randomforest" => new RandomForestRegressor()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setNumTrees(100)
        .setMaxDepth(10)
        
      case "gradientboosting" => new GBTRegressor()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setMaxIter(100)
        .setMaxDepth(8)
        .setStepSize(0.1)
        
      case "linearregression" => new LinearRegression()
        .setLabelCol(labelCol)
        .setFeaturesCol("features")
        .setMaxIter(100)
        .setRegParam(0.01)
        .setElasticNetParam(0.5)
        
      case _ => throw new IllegalArgumentException(s"Unsupported algorithm: $algorithmType")
    }
    
    val pipeline = new Pipeline().setStages(featurePreprocessor :+ regressor)
    
    val Array(trainData, testData) = data.randomSplit(Array(0.8, 0.2), seed = 42)
    val model = pipeline.fit(trainData)
    val predictions = model.transform(testData)
    val metrics = evaluateRegressionModel(predictions, labelCol)
    
    (model, metrics)
  }
  
  /**
   * Pipeline de clustering automatizado
   */
  def createClusteringPipeline(
    data: DataFrame,
    featureCols: Seq[String],
    numClusters: Int = 5,
    algorithmType: String = "kmeans"
  ): (PipelineModel, Map[String, Double]) = {
    
    val featurePreprocessor = createFeaturePreprocessor(data, featureCols)
    
    val clusterer = algorithmType.toLowerCase match {
      case "kmeans" => new KMeans()
        .setFeaturesCol("features")
        .setPredictionCol("cluster")
        .setK(numClusters)
        .setMaxIter(100)
        .setSeed(42)
        
      case "gaussianmixture" => new GaussianMixture()
        .setFeaturesCol("features")
        .setPredictionCol("cluster")
        .setK(numClusters)
        .setMaxIter(100)
        .setSeed(42)
        
      case "bisectingkmeans" => new BisectingKMeans()
        .setFeaturesCol("features")
        .setPredictionCol("cluster")
        .setK(numClusters)
        .setMaxIter(100)
        .setSeed(42)
        
      case _ => throw new IllegalArgumentException(s"Unsupported algorithm: $algorithmType")
    }
    
    val pipeline = new Pipeline().setStages(featurePreprocessor :+ clusterer)
    val model = pipeline.fit(data)
    val predictions = model.transform(data)
    val metrics = evaluateClusteringModel(predictions)
    
    (model, metrics)
  }
  
  /**
   * Sistema de recomendaÃ§Ã£o com ALS
   */
  def createRecommendationSystem(
    data: DataFrame,
    userCol: String,
    itemCol: String,
    ratingCol: String
  ): (ALSModel, Map[String, Double]) = {
    
    // Converter IDs para numÃ©ricos se necessÃ¡rio
    val userIndexer = new StringIndexer()
      .setInputCol(userCol)
      .setOutputCol("userIndex")
      .setHandleInvalid("skip")
    
    val itemIndexer = new StringIndexer()
      .setInputCol(itemCol)
      .setOutputCol("itemIndex")
      .setHandleInvalid("skip")
    
    val indexedData = itemIndexer.fit(userIndexer.fit(data).transform(data)).transform(
      userIndexer.fit(data).transform(data)
    )
    
    // Modelo ALS
    val als = new ALS()
      .setUserCol("userIndex")
      .setItemCol("itemIndex")
      .setRatingCol(ratingCol)
      .setRank(50)
      .setMaxIter(20)
      .setRegParam(0.1)
      .setAlpha(1.0)
      .setColdStartStrategy("drop")
      .setSeed(42)
    
    val Array(trainData, testData) = indexedData.randomSplit(Array(0.8, 0.2), seed = 42)
    val model = als.fit(trainData)
    
    // Avaliar modelo
    val predictions = model.transform(testData)
    val evaluator = new RegressionEvaluator()
      .setMetricName("rmse")
      .setLabelCol(ratingCol)
      .setPredictionCol("prediction")
    
    val rmse = evaluator.evaluate(predictions.filter(!col("prediction").isNaN))
    val metrics = Map("rmse" -> rmse)
    
    (model, metrics)
  }
  
  /**
   * Criar preprocessador de features
   */
  private def createFeaturePreprocessor(
    data: DataFrame,
    featureCols: Seq[String]
  ): Array[org.apache.spark.ml.PipelineStage] = {
    
    val schema = data.schema
    val stages = scala.collection.mutable.ArrayBuffer[org.apache.spark.ml.PipelineStage]()
    
    // Separar colunas por tipo
    val numericCols = featureCols.filter { col =>
      schema(col).dataType match {
        case _: NumericType => true
        case _ => false
      }
    }
    
    val stringCols = featureCols.filter { col =>
      schema(col).dataType match {
        case StringType => true
        case _ => false
      }
    }
    
    // Indexar colunas categÃ³ricas
    val indexedStringCols = stringCols.map { col =>
      val indexer = new StringIndexer()
        .setInputCol(col)
        .setOutputCol(s"${col}_indexed")
        .setHandleInvalid("skip")
      stages += indexer
      s"${col}_indexed"
    }
    
    // One-hot encoding para categÃ³ricas
    val encodedStringCols = indexedStringCols.map { col =>
      val encoder = new OneHotEncoder()
        .setInputCol(col)
        .setOutputCol(s"${col}_encoded")
      stages += encoder
      s"${col}_encoded"
    }
    
    // Imputar valores faltantes em colunas numÃ©ricas
    val imputedNumericCols = if (numericCols.nonEmpty) {
      val imputer = new Imputer()
        .setInputCols(numericCols.toArray)
        .setOutputCols(numericCols.map(_ + "_imputed").toArray)
        .setStrategy("mean")
      stages += imputer
      numericCols.map(_ + "_imputed")
    } else {
      numericCols
    }
    
    // Normalizar features numÃ©ricas
    val scaledNumericCols = if (imputedNumericCols.nonEmpty) {
      val scaler = new StandardScaler()
        .setInputCol("numeric_features")
        .setOutputCol("scaled_numeric_features")
        .setWithStd(true)
        .setWithMean(true)
      
      val numericAssembler = new VectorAssembler()
        .setInputCols(imputedNumericCols.toArray)
        .setOutputCol("numeric_features")
      
      stages += numericAssembler
      stages += scaler
      Seq("scaled_numeric_features")
    } else {
      Seq.empty
    }
    
    // Combinar todas as features
    val allFeatureCols = scaledNumericCols ++ encodedStringCols
    val finalAssembler = new VectorAssembler()
      .setInputCols(allFeatureCols.toArray)
      .setOutputCol("features")
    
    stages += finalAssembler
    stages.toArray
  }
  
  /**
   * Avaliar modelo de classificaÃ§Ã£o
   */
  private def evaluateClassificationModel(
    predictions: DataFrame,
    labelCol: String
  ): Map[String, Double] = {
    
    val evaluator = new MulticlassClassificationEvaluator()
      .setLabelCol(labelCol)
      .setPredictionCol("prediction")
    
    val accuracy = evaluator.setMetricName("accuracy").evaluate(predictions)
    val f1 = evaluator.setMetricName("f1").evaluate(predictions)
    val precision = evaluator.setMetricName("weightedPrecision").evaluate(predictions)
    val recall = evaluator.setMetricName("weightedRecall").evaluate(predictions)
    
    Map(
      "accuracy" -> accuracy,
      "f1" -> f1,
      "precision" -> precision,
      "recall" -> recall
    )
  }
  
  /**
   * Avaliar modelo de regressÃ£o
   */
  private def evaluateRegressionModel(
    predictions: DataFrame,
    labelCol: String
  ): Map[String, Double] = {
    
    val evaluator = new RegressionEvaluator()
      .setLabelCol(labelCol)
      .setPredictionCol("prediction")
    
    val rmse = evaluator.setMetricName("rmse").evaluate(predictions)
    val mae = evaluator.setMetricName("mae").evaluate(predictions)
    val r2 = evaluator.setMetricName("r2").evaluate(predictions)
    
    Map(
      "rmse" -> rmse,
      "mae" -> mae,
      "r2" -> r2
    )
  }
  
  /**
   * Avaliar modelo de clustering
   */
  private def evaluateClusteringModel(predictions: DataFrame): Map[String, Double] = {
    val evaluator = new ClusteringEvaluator()
      .setFeaturesCol("features")
      .setPredictionCol("cluster")
    
    val silhouette = evaluator.evaluate(predictions)
    
    Map("silhouette" -> silhouette)
  }
  
  /**
   * Hyperparameter tuning com cross-validation
   */
  def hyperparameterTuning(
    data: DataFrame,
    pipeline: Pipeline,
    paramGrid: Array[org.apache.spark.ml.param.ParamMap],
    evaluator: org.apache.spark.ml.evaluation.Evaluator,
    numFolds: Int = 3
  ): PipelineModel = {
    
    val cv = new CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(evaluator)
      .setEstimatorParamMaps(paramGrid)
      .setNumFolds(numFolds)
      .setParallelism(4)
      .setSeed(42)
    
    cv.fit(data)
  }
}
```

### ðŸŽ¯ CompetÃªncias Demonstradas

#### Big Data e Spark
- âœ… **Apache Spark**: Processamento distribuÃ­do em memÃ³ria
- âœ… **Scala AvanÃ§ado**: ProgramaÃ§Ã£o funcional e orientada a objetos
- âœ… **Streaming Real-time**: Kafka, Kinesis, processamento de streams
- âœ… **Data Lake**: Delta Lake, Iceberg, arquiteturas modernas

#### Machine Learning DistribuÃ­do
- âœ… **MLlib**: Machine learning escalÃ¡vel
- âœ… **Pipelines Automatizados**: Feature engineering, model training
- âœ… **Hyperparameter Tuning**: Cross-validation, grid search
- âœ… **Sistemas de RecomendaÃ§Ã£o**: ALS, collaborative filtering

#### Infraestrutura e DevOps
- âœ… **Kubernetes**: OrquestraÃ§Ã£o de containers
- âœ… **Docker**: ContainerizaÃ§Ã£o de aplicaÃ§Ãµes
- âœ… **Terraform**: Infrastructure as Code
- âœ… **Monitoramento**: Prometheus, Grafana, observabilidade

---

## ðŸ‡ºðŸ‡¸ English

### âš¡ Overview

Comprehensive **Big Data** platform developed with Scala and Apache Spark for large-scale distributed processing:

- ðŸ”¥ **Apache Spark**: In-memory distributed processing for batch and streaming
- ðŸ“Š **Advanced Analytics**: Machine Learning, Graph Analytics, distributed SQL
- ðŸŒŠ **Real-time Streaming**: Stream processing with Kafka and Kinesis
- ðŸ—„ï¸ **Data Lake**: Modern architecture with Delta Lake and Iceberg
- ðŸ”„ **Scalable ETL**: Distributed and fault-tolerant data pipelines
- ðŸ“ˆ **Monitoring**: Complete observability with metrics and alerts

### ðŸŽ¯ Platform Objectives

- **Process petabytes** of data with linear performance
- **Implement analytics** in real-time for instant decisions
- **Build data lakes** modern and governed
- **Automate pipelines** complex and critical data
- **Democratize big data** with simple APIs and clear documentation

### âš¡ Distributed Processing

#### 1. ðŸ”¥ Core Spark Framework
- Optimized Spark session manager
- Advanced DataFrame utilities
- Smart caching and partitioning
- Performance monitoring and optimization

#### 2. ðŸŒŠ Real-time Streaming
- Advanced Kafka stream processor
- State management for streams
- Windowed aggregations
- Real-time anomaly detection

#### 3. ðŸ“Š Advanced Analytics
- Distributed machine learning engine
- Automated ML pipelines
- Recommendation systems
- Hyperparameter tuning

### ðŸŽ¯ Skills Demonstrated

#### Big Data and Spark
- âœ… **Apache Spark**: In-memory distributed processing
- âœ… **Advanced Scala**: Functional and object-oriented programming
- âœ… **Real-time Streaming**: Kafka, Kinesis, stream processing
- âœ… **Data Lake**: Delta Lake, Iceberg, modern architectures

#### Distributed Machine Learning
- âœ… **MLlib**: Scalable machine learning
- âœ… **Automated Pipelines**: Feature engineering, model training
- âœ… **Hyperparameter Tuning**: Cross-validation, grid search
- âœ… **Recommendation Systems**: ALS, collaborative filtering

#### Infrastructure and DevOps
- âœ… **Kubernetes**: Container orchestration
- âœ… **Docker**: Application containerization
- âœ… **Terraform**: Infrastructure as Code
- âœ… **Monitoring**: Prometheus, Grafana, observability

---

## ðŸ“„ LicenÃ§a | License

Apache License 2.0 - veja o arquivo [LICENSE](LICENSE) para detalhes | see [LICENSE](LICENSE) file for details

## ðŸ“ž Contato | Contact

**GitHub**: [@galafis](https://github.com/galafis)  
**LinkedIn**: [Gabriel Demetrios Lafis](https://linkedin.com/in/galafis)  
**Email**: gabriel.lafis@example.com

---

<div align="center">

**Desenvolvido com â¤ï¸ para Big Data | Developed with â¤ï¸ for Big Data**

[![GitHub](https://img.shields.io/badge/GitHub-galafis-blue?style=flat-square&logo=github)](https://github.com/galafis)
[![Scala](https://img.shields.io/badge/Scala-DC322F?style=flat-square&logo=scala&logoColor=white)](https://www.scala-lang.org/)

</div>

