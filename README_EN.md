# Scala Spark Big Data

![Hero Image](images/hero_image.jpg)

## ðŸ‡¬ðŸ‡§ English

[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Scala](https://img.shields.io/badge/Scala-2.12-blue.svg)](https://www.scala-lang.org/)
[![Apache Spark](https://img.shields.io/badge/Apache_Spark-3.3.0-orange.svg)](https://spark.apache.org/)
[![sbt](https://img.shields.io/badge/sbt-1.x-green.svg)](https://www.scala-sbt.org/)

This repository presents a Big Data processing project using Scala and Apache Spark. The project demonstrates the implementation of ETL pipelines, batch and real-time data analysis, and the construction of machine learning models for fraud detection and customer segmentation.

### Features

*   **Batch Processing**: Ingestion, transformation, and loading of large volumes of transactional data.
*   **Streaming Processing**: Analysis of data streams for real-time anomaly detection and aggregations.
*   **Data Analysis & Machine Learning**: Implementation of machine learning algorithms for fraud detection, customer segmentation, time series analysis, and market basket analysis.
*   **Modular Structure**: Code organized into modules for easier maintenance and scalability.
*   **Flexible Configuration**: Use of configuration files to manage application and Spark parameters.

### Technologies Used

*   **Scala**: Main programming language.
*   **Apache Spark**: Unified framework for Big Data processing.
*   **Delta Lake**: Data storage layer for data lakes.
*   **sbt**: Build tool for Scala projects.
*   **Apache Hadoop (AWS S3)**: Integration with cloud storage.
*   **Logback**: Logging system.
*   **ScalaTest & Mockito**: Frameworks for unit testing.

### Project Structure

```
scala-spark-big-data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ scala/com/galafis/bigdata/
â”‚   â”‚       â”œâ”€â”€ analytics/        # Analytics and ML logic
â”‚   â”‚       â”œâ”€â”€ apps/             # Main applications
â”‚   â”‚       â”œâ”€â”€ core/             # Core components
â”‚   â”‚       â”œâ”€â”€ etl/              # Extraction, Transformation, and Loading pipelines
â”‚   â”‚       â”œâ”€â”€ models/           # Data model definitions
â”‚   â”‚       â”œâ”€â”€ monitoring/       # Monitoring tools
â”‚   â”‚       â”œâ”€â”€ storage/          # Storage abstraction layers
â”‚   â”‚       â”œâ”€â”€ streaming/        # Streaming processing logic
â”‚   â”‚       â””â”€â”€ utils/            # Utility functions
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/             # Unit tests
â”‚   â””â”€â”€ integration/      # Integration tests
â”œâ”€â”€ config/               # Application configuration files
â”œâ”€â”€ data/                 # Sample or mock data
â”œâ”€â”€ docs/                 # Additional documentation, diagrams
â”œâ”€â”€ images/               # Images and visual assets
â”œâ”€â”€ notebooks/            # Jupyter/Zeppelin notebooks for exploration
â”œâ”€â”€ project/              # sbt configurations
â”œâ”€â”€ build.sbt             # sbt build definitions
â”œâ”€â”€ README.md             # Portuguese README
â”œâ”€â”€ README_EN.md          # This file
â”œâ”€â”€ LICENSE               # Project license
â””â”€â”€ CONTRIBUTING.md       # Contribution guide
```

### How to Use

#### Prerequisites

*   Java Development Kit (JDK) 8 or higher
*   sbt (Scala Build Tool)
*   Apache Spark (for cluster execution, optional for `local[*]`) 

#### Building the Project

To build the project, navigate to the root directory and run:

```bash
sbt clean compile
```

#### Running Tests

To run unit and integration tests:

```bash
sbt test
```

To generate a code coverage report:

```bash
sbt coverageReport
```

#### Running the Application

The project can be run in different modes: `batch`, `streaming`, or `analytics`.

**Batch Mode:**

```bash
sbt "run batch"
```

**Streaming Mode:**

```bash
sbt "run streaming"
```

**Analytics Mode:**

```bash
sbt "run analytics"
```

### Architecture Diagram

```mermaid
graph TD
    A[Data Source] --> B(Data Ingestion)
    B --> C{ETL Processing}
    C --> D[Data Lake (Delta Lake)]
    D --> E(Batch Processing)
    D --> F(Streaming Processing)
    E --> G[ML Models & Analytics]
    F --> H[Real-time Anomaly Detection]
    G --> I[Results & Dashboards]
    H --> I
    I --> J[Users/Applications]

    subgraph Apache Spark
        C
        E
        F
        G
        H
    end

    subgraph AWS S3
        D
    end

    subgraph Scala Application
        A
        B
        I
        J
    end
```

### License

This project is licensed under the MIT License - see the `LICENSE` file for details.

### Author

**Gabriel Demetrios Lafis**

*   [GitHub](https://github.com/galafis)
*   [LinkedIn](https://www.linkedin.com/in/gabriel-demetrios-lafis/)

